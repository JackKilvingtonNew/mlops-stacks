## =====================================================================================================================
## =====================================================================================================================
##  Data Product Application Deployment Script for AWS CodeBuild executed within AWS CodePipeline.
##
##  Creates a Data Product cluster and Databricks job(s).
##
##  Inputs for this script are sourced from three locations:
##      1) AWS Secrets Manager (see secrets-manager key below)
##      2) AWS Parameter Store (see parameter-store key below)
##      3) AWS CodePipeline Environment Variables (see description immediately below)
##
##  Required AWS CodePipeline Environment Variables:
##      $env                                          the environment identifier for the environment being deployed to e.g. sbx
##      $codeartifact_domain:                         the domain name of the AWS CodeArtifact repository e.g. edp
##      $codeartifact_domain_owner                    the AWS account identifier for the owner of the AWS CodeArtifact domain e.g. 851725496338
##      $data_domain_name:                            the name of the data domain within the Arqiva business e.g. utilities.
##      $product_local_name:                          the local, or shorthand name of the data product e.g. ntwk-covg-sdp
##      $product_fq_name:                             the fully qualified name for the data product, e.g. utilities-ntwk-covg-sdp
##      $product_app_local_name:                      the local, or shorthand name of the data product application e.g. ntwk-covg-sdp-etl
##      $product_app_current_version_ssm_param_name   the AWS Systems Manager Parameter Store param for holding the deployed version
##      $region:                                      the AWS region e.g. eu-west-2
##      $sp_admin_asm_secret_arn:                     the AWS Secrets Manager ARN for administrator service principal secret.
##      $sp_data_product_asm_secret_arn:              the AWS Secrets Manager ARN for data product service principal secret.
##
##  Identifying what version of the artefact is to be deployed:
##    This script derives the version of the artefact to be deployed from the version value specified inside the
##    pyproject.toml file of the project.  It uses this value in outbound requests to AWS CodeArtifact to retrieve
##    this package version and deploy it accordingly.
##
##  AWS CodeBuild Reference Doc:
##      https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html
## =====================================================================================================================
## =====================================================================================================================

#TODO: update to work with mlops folder structure
version: 0.2

env:
  secrets-manager:
    sp_data_product_client_id: "${sp_data_product_asm_secret_arn}:client_id"
    sp_data_product_client_secret: "${sp_data_product_asm_secret_arn}:secret"
    sp_admin_client_id: "${sp_admin_asm_secret_arn}:client_id"
    sp_admin_client_secret: "${sp_admin_asm_secret_arn}:secret"
  parameter-store:
    account_id: "/edp/${env}/databricks/account_id"
    workspace_id: "/edp/${env}/databricks/workspace_id"
    workspace_hostname: "/edp/${env}/databricks/host"
    cluster_init_scripts_path: "/edp/${env}/databricks/init_scripts_s3_path"
    cluster_init_scripts: "/edp/${env}/databricks/init_scripts"
    cluster_log_conf: "/edp/${env}/${data_domain_name}/${product_local_name}/cluster_log_conf"
    cluster_custom_tags: "/edp/${env}/${data_domain_name}/${product_local_name}/custom_tags"
    cluster_instance_profile_arn: "/edp/${env}/${data_domain_name}/${product_local_name}/instance_profile_arn"
  variables:
    common_repository_name: "product-common"
    helper_scripts_path: "common/scripts/databricks/helpers/"
    poetry_version: 1.8.2

phases:
  install:
    runtime-versions:
      python: 3.11.0
    commands:
      - |
        echo "[+] Installing Python Poetry"
        pip install poetry==${poetry_version}
      - |
        echo "[+] Installing the Databricks CLI"
        curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
      - |
        echo "[+] Preparing helper scripts for use in deployment"
        cp -r ${helper_scripts_path}helpers/* "." && cp -r ${helper_scripts_path}pyproject.toml "."
        poetry install --no-root

  pre_build:
    on-failure: ABORT
    commands:
      - |
        echo "[+] Setting up Databricks configuration"
        echo "[DEFAULT]
        host = ${workspace_hostname}
        account_id = ${account_id}
        client_id = ${sp_admin_client_id}
        client_secret = ${sp_admin_client_secret}" > ~/.databrickscfg
      - |
        echo "[+] Identifying artefact version to deploy"
        poetry version --directory=domains/${data_domain_name}/${product_local_name}/app/${product_app_local_name}
        export ARTIFACT_NAME=$(poetry version --directory=domains/${data_domain_name}/${product_local_name}/app/${product_app_local_name} | awk '{print $1}')
        export ARTIFACT_VERSION=$(poetry version --directory=domains/${data_domain_name}/${product_local_name}/app/${product_app_local_name} | awk '{print $2}')
        echo "Identified ${ARTIFACT_NAME} with version ${ARTIFACT_VERSION} to deploy."
      - |
        echo "[+] Logging into AWS CodeArtifact repository named ${product_fq_name}"
        aws codeartifact login --tool pip \
          --repository "${product_fq_name}" \
          --domain "${codeartifact_domain}" \
          --domain-owner "${codeartifact_domain_owner}" \
          --region "${region}"
        echo "Logged in."
      - |
        echo "[+] Generating a data product-specific cluster initialisation script from our pre-defined template.  Init script will install ${ARTIFACT_VERSION} of ${ARTIFACT_NAME}"
        poetry run python ./replace.py \
          common/scripts/databricks/templates/init_script_template init_script \
          'VERSION' "${ARTIFACT_VERSION}" \
          'PRODUCT_LOCAL_NAME' "${product_local_name}" \
          'PRODUCT_APP_LOCAL_NAME' "${product_app_local_name}" \
          'REPOSITORY' "${product_fq_name}" \
          'REGION' "${region}" \
          'DOMAIN' "${codeartifact_domain}" \
          'DOMAIN_OWNER' "${codeartifact_domain_owner}" \
          'COMMON_REPOSITORY' "${common_repository_name}"
        echo "The generated Init script contents are as follows:"
        cat init_script
      - |
        echo "[+] Uploading the generated Data Product-specific cluster initialisation script to S3 bucket at ${init_script_s3_path}"
        export base_s3_config_path="${cluster_init_scripts_path}/${env}/${product_fq_name}"
        export init_script_s3_path="${base_s3_config_path}/init_script"
        aws s3 cp init_script "${init_script_s3_path}"
      - |
        echo "[+] Uploading the Data Product-specific application configuration file to S3 bucket at ${app_config_s3_path}"
        export app_config_s3_path="${base_s3_config_path}/app-config.${env}.yaml"
        export app_config_local_path="domains/utilities/${product_local_name}/app/${product_app_local_name}/config/app-config.${env}.yaml"
        aws s3 cp ${app_config_local_path} ${app_config_s3_path}

  build:
    on-failure: ABORT
    commands:
      - |
        echo "[+] Deploying Data Product-specific Databricks cluster"
        poetry run python ./deploy_cluster.py \
          --cluster-name "${product_fq_name}" \
          --cluster-instance-profile-arn "${cluster_instance_profile_arn}" \
          --custom-tags "${cluster_custom_tags}" \
          --cluster-log-conf "${cluster_log_conf}" \
          --cluster-init-script "${cluster_init_scripts}" \
          --cluster-custom-init-script "${init_script_s3_path}" \
          --sp-data-product-client-id "${sp_data_product_client_id}" \
          --json-template-file-path "common/scripts/databricks/templates/job_cluster_template.json"
      - |
        echo "[+] Deploying Data Product-specific Databricks Job(s)"
        poetry run python ./deploy_job.py \
          --app-config-path "${app_config_local_path}" \
          --default-template-path "common/scripts/databricks/templates/job_create_template.json" \
          --cluster-name "${product_fq_name}" \
          --user-to-run "${sp_data_product_client_id}" \
          --s3-config-path "${app_config_s3_path}" \
          --product-name "${product_local_name}" \
          --data-domain "${data_domain_name}" \
          --product-app-name "${product_app_local_name}"
    finally:
      - |
        echo "[+] Cleaning up Databricks credentials file"
        rm -rf ~/.databrickscfg

  post_build:
    commands:
      - |
        echo "[+] Saving the deployed version of ${product_app_local_name} app in SSM Parameter store"
        aws ssm put-parameter --name "${product_app_current_version_ssm_param_name}" --value "${ARTIFACT_VERSION}" --overwrite
      - echo "Deployment of ${product_app_local_name} app with version ${ARTIFACT_VERSION} completed on $(date)"

cache:
  paths:
    - '/root/.cache/pip'